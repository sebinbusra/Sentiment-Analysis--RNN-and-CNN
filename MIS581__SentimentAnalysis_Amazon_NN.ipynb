{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIS581_ SentimentAnalysis_Amazon_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install glove-python-binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJxmJokd7D-b",
        "outputId": "7c9b924f-cb49-4527-d995-e9e78506c981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove-python-binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 204 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 948 kB 25.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "from bs4 import BeautifulSoup  \n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from snowballstemmer import TurkishStemmer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import glove \n",
        "from scipy import spatial\n",
        "from glove import Corpus, Glove\n",
        "from multiprocessing import Pool\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L65SkKpVQLQZ",
        "outputId": "e89eb67e-0384-4379-e6fb-4fe5b7f5a62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "jbiFF8Dxnh3q",
        "outputId": "3f5e81e0-a6ac-45d2-939c-a9e5b4ad7383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-54c7e1ed-7ddb-464d-bcd8-d20fa9324704\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-54c7e1ed-7ddb-464d-bcd8-d20fa9324704\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving deneme1016_scraped_data.csv to deneme1016_scraped_data (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded['deneme1016_scraped_data.csv']))"
      ],
      "metadata": {
        "id": "VYjxXfGLnm-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis \n",
        "df.head()\n",
        "df.columns\n",
        "print('length of data is', len(df))\n",
        "df. shape\n",
        "df.info()\n",
        "df.dtypes\n",
        "np.sum(df.isnull().any(axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYUMKViinqz1",
        "outputId": "26aed277-b5fe-4382-b564-4cd0006ba35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data is 20160\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20160 entries, 0 to 20159\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   stars    20160 non-null  object\n",
            " 1   comment  20160 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 315.1+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting stars from stars column\n",
        "stars = df[\"stars\"]\n",
        "new_stars = []\n",
        "def star_list(x):\n",
        "    x = list(x)\n",
        "    for i in range(len(x)):\n",
        "        new_stars.append(x[i][-3:-2])\n",
        "    return new_stars\n",
        "    \n",
        "stars_no = star_list(stars)\n",
        "stars_column = pd.Series(stars_no)\n",
        "df['stars'] = stars_column\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-uxSIf0xeYyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0d1cb6f2-fe9a-47b3-e9bc-ad700f9ecd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  stars                                            comment\n",
              "0     1  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bu ürünü aldığımda...\n",
              "1     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Fiyatı da uygun, k...\n",
              "2     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Son kullanma tarih...\n",
              "3     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Earl grey aromali ...\n",
              "4     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bir çay bağımlısı ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd79328f-b365-4ca8-bd55-7313e3c27eeb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bu ürünü aldığımda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Fiyatı da uygun, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Son kullanma tarih...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Earl grey aromali ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bir çay bağımlısı ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd79328f-b365-4ca8-bd55-7313e3c27eeb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd79328f-b365-4ca8-bd55-7313e3c27eeb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd79328f-b365-4ca8-bd55-7313e3c27eeb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns sentiment value based on the overall ratings\n",
        "def generate_sentiment(row):\n",
        "    if row['stars'] == '3' :\n",
        "        val = 'Neutral'\n",
        "    elif row['stars'] == '1' or row['stars'] == '2':\n",
        "        val = 'Negative'\n",
        "    elif row['stars'] == '4' or row['stars'] == '5':\n",
        "        val = 'Positive'\n",
        "    else:\n",
        "        val = -1\n",
        "    return val\n",
        "\n",
        "df['sentiment'] = df.apply(generate_sentiment, axis=1)\n",
        "df['sentiment'].value_counts()\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A1UqtWftea_A",
        "outputId": "254e821d-089a-4dcd-ed63-5cf642a233b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  stars                                            comment sentiment\n",
              "0     1  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bu ürünü aldığımda...  Negative\n",
              "1     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Fiyatı da uygun, k...  Positive\n",
              "2     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Son kullanma tarih...  Positive\n",
              "3     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Earl grey aromali ...  Positive\n",
              "4     5  \\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bir çay bağımlısı ...  Positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2dcf1207-2922-4e16-ad6e-337c363c0cf4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bu ürünü aldığımda...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Fiyatı da uygun, k...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Son kullanma tarih...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Earl grey aromali ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Bir çay bağımlısı ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dcf1207-2922-4e16-ad6e-337c363c0cf4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2dcf1207-2922-4e16-ad6e-337c363c0cf4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2dcf1207-2922-4e16-ad6e-337c363c0cf4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['stars'].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQHRsMH4B0m3",
        "outputId": "85d344c7-1c21-468f-e880-b460ab2854f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    0.614931\n",
              "4    0.140575\n",
              "3    0.106994\n",
              "1    0.079663\n",
              "2    0.057837\n",
              "Name: stars, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "PQxbGxfkdFX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Text\n",
        "def cleantext(raw_text, remove_stopwords=True, stemming=True, split_text = False):\n",
        "    text1 = BeautifulSoup(raw_text, 'lxml').get_text()  # remove html\n",
        "    text2 = re.sub('[0123456789,/\\.!?:‘’()\"]', '', text1) # remove non-character\n",
        "    words = text2.lower().split() # convert to lower case \n",
        "    if remove_stopwords:\n",
        "      stop_word_list = nltk.corpus.stopwords.words('turkish') # removing stopwords\n",
        "      words = [w for w in words if not w in stop_word_list]\n",
        "    if stemming == True: # stemming Turkish words\n",
        "      turkStem = TurkishStemmer()\n",
        "      words = [turkStem.stemWord(word) for word in words]\n",
        "    if split_text == True: # split text\n",
        "      return (words)\n",
        "    return(\" \".join(words))"
      ],
      "metadata": {
        "id": "U6PecjwSnxCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['comment']\n",
        "X_cleaned = []\n",
        "for review in reviews:\n",
        "   X_cleaned.append(cleantext(review))"
      ],
      "metadata": {
        "id": "EB5ON2q_Sq1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split review text into parsed sentences using NLTK's punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def Text_to_Sentences(review, tokenizer, remove_stopwords=False):\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleantext(raw_sentence, remove_stopwords, split_text=True))\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "wERC1njJYLVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for review in X_cleaned:\n",
        "  sentences += Text_to_Sentences(review, tokenizer)"
      ],
      "metadata": {
        "id": "s5js5ojBhH5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def track_vocab(sentences, verbose =  True):\n",
        "    vocab = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "                \n",
        "    return vocab"
      ],
      "metadata": {
        "id": "tV-DQmkRybO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the occurrence of all words in the data\n",
        "vocab_count = track_vocab(sentences)\n",
        "print({k: vocab_count[k] for k in list(vocab_count)[:5]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je3VDk0iysGA",
        "outputId": "173357a5-a7c8-4902-974f-46208b450e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ür': 4817, 'al': 6002, 'ber': 424, 'tadı': 2391, 'bir': 26783}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lookup_tables(vocab_count):\n",
        "    \n",
        "    # sorting the words from most to least frequent in text occurrence\n",
        "    sorted_vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
        "    # create vocab_to_int dictionary\n",
        "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "    \n",
        "    # return tuple\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(vocab_count)"
      ],
      "metadata": {
        "id": "Gt-50YMYy0Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the data\n",
        "sentence_ints = []\n",
        "for sentence in sentences:\n",
        "    sentence_ints.append([vocab_to_int[word] for word in sentence])"
      ],
      "metadata": {
        "id": "RI7rLRi2y7R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', sentence_ints[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACvHSbA5xfpc",
        "outputId": "ca5c36fb-fcbe-4fb0-e5e0-1f3b0c6289de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words:  1610\n",
            "Tokenized review: \n",
            " [[8, 4, 348, 30, 0, 492, 2, 493, 44, 494, 495, 11, 95, 257, 340, 496, 497, 11, 250, 498, 499, 231, 307, 500, 140, 501, 183, 502, 38, 503, 50, 284, 232, 504, 345, 280, 231, 232, 505, 506, 507, 308, 73, 127, 175, 508, 0, 346, 509, 510, 45]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(sentences_token, seq_length):\n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(sentences_token), seq_length), dtype=int)\n",
        "\n",
        "    # for each title, I grab that title and \n",
        "    for i, row in enumerate(sentences_token):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features\n",
        "\n",
        "# pad the titles\n",
        "seq_length = 50\n",
        "features = pad_features(sentence_ints, seq_length)"
      ],
      "metadata": {
        "id": "pseD_22izFtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['sentiment'] \n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J53mUJTO3KoH",
        "outputId": "a530aa7c-c2d6-4a5b-8ff6-e84f2ad2ed5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Negative\n",
            "1        Positive\n",
            "2        Positive\n",
            "3        Positive\n",
            "4        Positive\n",
            "           ...   \n",
            "20155    Positive\n",
            "20156    Positive\n",
            "20157    Positive\n",
            "20158    Positive\n",
            "20159    Positive\n",
            "Name: sentiment, Length: 20160, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace labels with integers\n",
        "dict_labels = {'Negative':0, 'Neutral':1, 'Positive': 2}\n",
        "labels = pd.Series(labels).replace(dict_labels)\n",
        "labels = labels.to_numpy(copy = True)"
      ],
      "metadata": {
        "id": "rYEwYZKg2zqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec Embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "7zSbZmgnc19L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences = sentences, size = 300, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
        "model.init_sims(replace = True)\n",
        "model.save('word2vec_model')\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(model.wv.index2word))\n",
        "print(\"Show first 10 words in the vocabulary list: \\n\", model.wv.index2word[0:10])\n",
        "# Load trained Word2Vec model\n",
        "model = Word2Vec.load('word2vec_model')\n",
        "\n",
        "# Get Word2Vec embedding matrix\n",
        "word2vec_embed_matrix = model.wv.syn0 \n",
        "print(\"Shape of embedding matrix : \", word2vec_embed_matrix.shape)\n",
        "print(word2vec_embed_matrix)"
      ],
      "metadata": {
        "id": "Hvyben9ooB24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b3bedb-6930-456b-bb84-4f97a9ff70ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the vocabulary list : 1610 \n",
            "\n",
            "Show first 10 words in the vocabulary list: \n",
            " ['bir', 'kitap', 'ol', 'amazo', 'al', 'gel', 'kargo', 'kahve', 'ür', 'fiyat']\n",
            "Shape of embedding matrix :  (1610, 300)\n",
            "[[-0.06131336 -0.01756017 -0.04114821 ...  0.07660954 -0.01107388\n",
            "  -0.02477599]\n",
            " [ 0.01701378 -0.01741518 -0.02642088 ...  0.09859084 -0.0417672\n",
            "  -0.10511018]\n",
            " [ 0.03217573 -0.03930807 -0.05874291 ... -0.03861464 -0.0320034\n",
            "   0.00843136]\n",
            " ...\n",
            " [-0.00347647  0.04661679 -0.14094475 ...  0.03727179 -0.05411803\n",
            "  -0.09462743]\n",
            " [ 0.06006055 -0.0681548  -0.09083479 ...  0.00026503  0.0009223\n",
            "  -0.03580105]\n",
            " [ 0.0764944  -0.07224622 -0.06080306 ...  0.01526372 -0.01336818\n",
            "  -0.03159595]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Word2Vec Dictionary \n",
        "word2vec_dict = dict({})\n",
        "for idx, key in enumerate(model.wv.vocab):\n",
        "    word2vec_dict[key] = model.wv[key]"
      ],
      "metadata": {
        "id": "wnTuhJxgD45a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove Embeddings"
      ],
      "metadata": {
        "id": "UTJ__LKBc6ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Corpus instance\n",
        "corpus = Corpus()\n",
        "# training the corpus to generate the co-occurence matrix which is used in GloVe\n",
        "corpus.fit(sentences, window = 3) \n",
        "#creating a Glove object which will use the matrix created in the above lines\n",
        "glove = Glove(no_components = 300, learning_rate = 0.05)\n",
        "# Train model\n",
        "glove.fit(matrix = corpus.matrix, epochs = 30, no_threads = Pool()._processes, verbose = True)\n",
        "# Save and load model\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ru8Psocr55c",
        "outputId": "6ed81bd2-10a0-41da-9be9-c4cc6a984ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 30 training epochs with 2 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n",
            "Epoch 21\n",
            "Epoch 22\n",
            "Epoch 23\n",
            "Epoch 24\n",
            "Epoch 25\n",
            "Epoch 26\n",
            "Epoch 27\n",
            "Epoch 28\n",
            "Epoch 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model = glove.load(\"glove_model\")\n",
        "glove_embed_matrix = glove_model.word_vectors\n",
        "print(\"Vector Representation for the word alışveriş in Glove:\")\n",
        "np.array(glove_model.word_vectors[glove_model.dictionary['alışveriş']]).reshape(1,300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm2jilp0ZN8l",
        "outputId": "a43e856f-9f9e-4e46-8e88-29422c079ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Representation for the word alışveriş in Glove:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.14627044e-02,  7.94729168e-02, -1.54854273e-01,\n",
              "        -6.87758843e-03, -1.00538478e-01,  4.01113893e-02,\n",
              "         1.29170042e-01,  3.12598159e-02, -1.66763067e-01,\n",
              "         9.06033295e-02, -2.70750069e-05, -1.45088151e-01,\n",
              "         5.58917038e-02, -8.52732500e-02,  1.25663393e-02,\n",
              "        -1.29933471e-01,  7.41891042e-02,  8.13972306e-02,\n",
              "        -6.55204516e-02,  1.35513398e-01, -9.92604186e-02,\n",
              "         1.57667148e-01,  1.66576146e-01,  9.87070127e-02,\n",
              "         7.47653958e-02, -1.84725478e-01, -2.95330495e-03,\n",
              "         9.67495717e-02,  5.20756294e-02, -1.18573324e-01,\n",
              "        -5.67743345e-02, -9.64702836e-02, -1.23140686e-01,\n",
              "        -3.93832980e-03,  4.91984458e-02,  5.62260471e-03,\n",
              "         4.62605275e-02,  1.96246717e-02,  1.80406849e-01,\n",
              "        -9.75186135e-02,  6.20987737e-02,  3.75416419e-02,\n",
              "         1.28134980e-02,  2.78962646e-02, -1.08896986e-01,\n",
              "         1.30806558e-01, -9.98335092e-02, -1.64244363e-01,\n",
              "         1.11729163e-01, -6.77652915e-02,  1.08143766e-01,\n",
              "         1.61496625e-01,  1.46173590e-01, -1.77622739e-01,\n",
              "        -1.53230565e-01,  1.19512672e-01,  6.12246820e-02,\n",
              "        -2.78506079e-02,  1.69134000e-01, -1.52874202e-01,\n",
              "         1.55718276e-01,  1.25679040e-01,  1.67134991e-01,\n",
              "         1.03424015e-01,  1.13051884e-01,  1.73116876e-01,\n",
              "         6.28659659e-02,  1.27032409e-01,  1.86693087e-01,\n",
              "         1.13498729e-01,  9.06888325e-02,  9.03230809e-02,\n",
              "         2.56290905e-03, -1.59568242e-01, -1.07289954e-01,\n",
              "        -9.01493651e-02, -4.18188636e-02, -4.97127296e-02,\n",
              "         1.70171871e-01, -1.09832403e-02, -1.30350529e-01,\n",
              "         8.62511681e-02,  1.75067105e-01,  7.46683943e-02,\n",
              "         1.92954759e-02, -1.52007263e-01,  1.17512027e-01,\n",
              "        -1.45569371e-01, -2.59756109e-02,  8.81011603e-02,\n",
              "        -1.17433436e-01, -6.64334718e-02, -1.32759293e-01,\n",
              "         1.03918011e-01, -6.69164483e-02,  2.02216665e-01,\n",
              "        -9.10680397e-02,  2.00142211e-03, -1.17310408e-01,\n",
              "        -7.44080236e-02, -8.57807269e-02, -8.22831404e-02,\n",
              "        -2.05290140e-01,  1.52735259e-01,  1.07242657e-01,\n",
              "        -1.76663140e-01,  1.21970101e-01, -1.45262676e-01,\n",
              "         1.82295552e-01, -1.49026872e-01, -1.03441725e-01,\n",
              "         1.41728888e-01, -1.21299801e-01, -1.30095926e-01,\n",
              "         1.50574599e-01,  6.15746145e-02, -1.84479188e-01,\n",
              "        -9.05805925e-02, -5.54939258e-02,  1.58881480e-01,\n",
              "         8.27512617e-02,  2.70516494e-02,  1.44753446e-01,\n",
              "         5.91045137e-02, -1.49392558e-01,  2.16736609e-02,\n",
              "         2.78937608e-02, -1.77761544e-01, -1.73695877e-01,\n",
              "        -1.89597286e-01, -1.31345118e-01,  6.47202716e-02,\n",
              "        -1.29896189e-01, -1.18685432e-01,  8.22776743e-02,\n",
              "         1.40559931e-01, -2.40269454e-02,  6.73948546e-02,\n",
              "         1.00467234e-01,  8.41681320e-02,  1.43850622e-02,\n",
              "        -4.47812081e-02, -7.51146678e-02, -1.19796229e-01,\n",
              "        -7.18166942e-02,  1.87569015e-02,  1.02747315e-01,\n",
              "         1.34440526e-01, -1.53528355e-02, -5.20734398e-02,\n",
              "        -1.59414197e-01,  1.20354710e-01, -1.51027921e-01,\n",
              "         1.28682527e-01,  7.10486539e-02,  1.14525756e-01,\n",
              "         8.36388217e-03,  1.46275928e-01, -1.87521922e-01,\n",
              "         8.36853981e-02, -2.62037937e-01,  1.14323956e-01,\n",
              "        -4.77443357e-02,  1.51597817e-01,  2.33985668e-02,\n",
              "         8.36738815e-02, -1.46232434e-01,  1.53631729e-01,\n",
              "         4.89123928e-02, -1.15279213e-01, -1.70285884e-01,\n",
              "        -5.11409501e-02, -1.33022268e-01,  1.37496907e-01,\n",
              "         1.45611052e-01, -7.43862275e-02,  1.74597276e-01,\n",
              "         1.38186525e-01, -2.06239316e-01,  5.94171064e-02,\n",
              "         1.64949971e-02, -1.37732368e-01,  1.65673007e-01,\n",
              "         3.74225721e-02, -1.53500947e-01,  1.20045964e-01,\n",
              "        -1.34933320e-01,  1.18471248e-01,  7.74552275e-02,\n",
              "         7.60202599e-02,  1.26317759e-01, -1.08516967e-01,\n",
              "         4.49496367e-02,  1.46454354e-01,  2.94072662e-02,\n",
              "         8.47764452e-02,  1.23718298e-02,  1.19255629e-02,\n",
              "        -6.78616341e-02, -1.28854563e-01,  6.22745670e-02,\n",
              "         9.40904042e-02, -1.39106437e-01, -9.34688389e-02,\n",
              "         1.51951176e-01, -1.49324165e-01, -8.14087764e-02,\n",
              "        -2.99856353e-02,  1.13016694e-01, -1.13225149e-01,\n",
              "         5.41335884e-02, -1.67995056e-01, -4.07470608e-02,\n",
              "         3.60904771e-02, -1.36858713e-01,  1.18315327e-01,\n",
              "        -1.24186629e-01, -1.16065735e-01,  1.33531965e-01,\n",
              "        -1.08058783e-01,  2.35842964e-02,  1.13624526e-01,\n",
              "         1.51813020e-01, -1.14711018e-01, -1.39499941e-01,\n",
              "        -1.33255291e-01,  8.30921697e-02,  3.59011773e-02,\n",
              "        -4.93779897e-02, -1.60023097e-01,  7.00604619e-02,\n",
              "        -1.40838702e-01,  2.66900910e-02, -3.75883761e-02,\n",
              "         1.87177225e-02, -8.31377402e-02,  1.00300978e-01,\n",
              "         1.63948704e-01, -1.07475609e-01, -1.12405757e-01,\n",
              "         1.49110777e-01,  4.77890798e-02,  1.58769114e-01,\n",
              "        -1.69570780e-01, -1.08215191e-01,  1.47665673e-01,\n",
              "        -1.65383144e-01, -5.01286478e-02,  1.86180106e-01,\n",
              "        -1.06925168e-01,  6.86906675e-02, -7.47883299e-02,\n",
              "        -1.12487685e-01, -9.96189396e-02, -4.87717259e-02,\n",
              "        -1.71666587e-01,  1.05255429e-01,  9.21829097e-02,\n",
              "        -8.80752911e-02, -3.03149516e-02, -1.30817291e-01,\n",
              "        -7.15754415e-02,  4.59074127e-02, -1.78840192e-01,\n",
              "         1.52671271e-01,  1.88857190e-01,  5.43694955e-02,\n",
              "        -6.07350806e-02,  5.10858006e-02,  1.84847624e-01,\n",
              "         1.54561018e-01,  6.30734544e-02, -1.26754385e-01,\n",
              "        -1.05560523e-01, -1.00436500e-01,  8.09111013e-02,\n",
              "        -1.76732381e-02,  1.51111383e-01, -1.33779692e-01,\n",
              "        -1.77875878e-01, -9.93246563e-02, -2.20251210e-02,\n",
              "        -3.58691505e-02, -1.10789349e-01, -1.44945920e-01,\n",
              "        -7.15377911e-02, -4.88681254e-02,  7.96109643e-02,\n",
              "        -1.04715864e-01,  2.14673732e-02,  1.29527840e-01,\n",
              "        -1.43948701e-01,  1.62582948e-01,  1.11149481e-01,\n",
              "         1.67238704e-01, -1.18069860e-01, -4.83147771e-02,\n",
              "         1.99239730e-01,  1.88616261e-01, -1.42285543e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText Embeddings"
      ],
      "metadata": {
        "id": "_Ff-xRRv1bYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!make\n",
        "!cp fasttext ../\n",
        "%cd ..\n",
        "!pip install fasttext\n",
        "import fasttext "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX5Bs8Fz1Yvh",
        "outputId": "4f5b7bc5-4115-487e-9ce5-6e11fb1767f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 3930 (delta 29), reused 70 (delta 29), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.33 MiB | 33.33 MiB/s, done.\n",
            "Resolving deltas: 100% (2446/2446), done.\n",
            "/content/fastText\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/args.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/autotune.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/matrix.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/dictionary.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/loss.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/productquantizer.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/densematrix.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/quantmatrix.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/vector.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/model.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/utils.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/meter.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG -c src/fasttext.cc\n",
            "c++ -pthread -std=c++11 -march=native -O3 -funroll-loops -DNDEBUG args.o autotune.o matrix.o dictionary.o loss.o productquantizer.o densematrix.o quantmatrix.o vector.o model.o utils.o meter.o fasttext.o src/main.cc -o fasttext\n",
            "/content\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3145633 sha256=3579ec35df0935296f123623b998f52ee6f577a0ad54a121dac2fb849fa43803\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download, extract and load Fasttext word embedding model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cphJH_a1nmE",
        "outputId": "d4150573-63ed-4084-a611-186e2549ca5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-22 11:02:12--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4506977940 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.tr.300.bin.gz’\n",
            "\n",
            "cc.tr.300.bin.gz    100%[===================>]   4.20G  45.8MB/s    in 1m 42s  \n",
            "\n",
            "2022-05-22 11:03:55 (42.0 MB/s) - ‘cc.tr.300.bin.gz’ saved [4506977940/4506977940]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download, extract and load Fasttext word embedding vectors\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\n",
        "# !gunzip cc.tr.300.vec.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdPkNyRhMk4x",
        "outputId": "98f4568f-545a-4e7f-fea3-390887668460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-22 08:23:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1261500728 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.tr.300.vec.gz’\n",
            "\n",
            "cc.tr.300.vec.gz    100%[===================>]   1.17G  38.4MB/s    in 32s     \n",
            "\n",
            "2022-05-22 08:24:32 (37.8 MB/s) - ‘cc.tr.300.vec.gz’ saved [1261500728/1261500728]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('tr', if_exists='ignore')\n",
        "ft_model = fasttext.load_model('cc.tr.300.bin')\n",
        "ft_model.save_model('ft_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qQgwypx72r2",
        "outputId": "9d18ae03-ddee-42bd-9cd1-61d0ed4d9c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.get_word_vector(\"büşra\")\n",
        "ft_model.get_nearest_neighbors('zeki')"
      ],
      "metadata": {
        "id": "21ypBO2B86Kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2048832-b20d-42f7-c31d-403dce212220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.7558695673942566, 'zeki.'),\n",
              " (0.7393233776092529, 'zekii'),\n",
              " (0.7049357891082764, 'yetenekli'),\n",
              " (0.6554639935493469, 'becerikli'),\n",
              " (0.6511648893356323, 'zekisi'),\n",
              " (0.650030255317688, 'zekiler'),\n",
              " (0.6492676138877869, 'zekidir'),\n",
              " (0.6472271084785461, 'çalışkan'),\n",
              " (0.647098183631897, 'meziyetli'),\n",
              " (0.6430851221084595, 'zekiyse')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
        "    return data"
      ],
      "metadata": {
        "id": "VIu3Qkpk7jaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "    ft_embed_matrix = load_vectors('cc.tr.300.vec')"
      ],
      "metadata": {
        "id": "BQOt8zgrbI-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Embedding Layer"
      ],
      "metadata": {
        "id": "DVZKbhE4-f6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "metadata": {
        "id": "B6BVIep_AwvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Word2vec Weights Matrix"
      ],
      "metadata": {
        "id": "jGNwd5ncBQZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
        "target_vocab = sorted_vocab\n",
        "emb_dim = 300\n",
        "\n",
        "matrix_len = len(target_vocab)\n",
        "word2vec_weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(target_vocab):\n",
        "    try: \n",
        "        word2vec_weights_matrix[i] = word2vec_dict[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        word2vec_weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
      ],
      "metadata": {
        "id": "ImBisVkiBVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Glove Weights Matrix"
      ],
      "metadata": {
        "id": "b567sllmAn5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
        "target_vocab = sorted_vocab\n",
        "emb_dim = 300\n",
        "\n",
        "matrix_len = len(target_vocab)\n",
        "glove_weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(target_vocab):\n",
        "    try: \n",
        "        glove_weights_matrix[i] = glove_model.dictionary[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        glove_weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
      ],
      "metadata": {
        "id": "Wk5wSRn3AkF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For FastText Weight Matrix"
      ],
      "metadata": {
        "id": "HOhiqJ-zCMgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
        "target_vocab = sorted_vocab\n",
        "emb_dim = 300\n",
        "\n",
        "matrix_len = len(target_vocab)\n",
        "ft_weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(target_vocab):\n",
        "    try: \n",
        "        ft_weights_matrix[i] = ft_model.get_word_vector(word)\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        ft_weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
      ],
      "metadata": {
        "id": "Wx4-dIx8CPpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "iJ_sWMbtEU9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Word2vec Embeddings"
      ],
      "metadata": {
        "id": "pEiPdN_t4Fwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "weights_matrix = word2vec_weights_matrix\n",
        "output_size = 3\n",
        "hidden_dim = 128\n",
        "drop_prob = 0.1"
      ],
      "metadata": {
        "id": "Y5pFkFhK4IMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Glove Embeddings"
      ],
      "metadata": {
        "id": "J9B4U62K4JEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "weights_matrix = glove_weights_matrix \n",
        "output_size = 3\n",
        "hidden_dim = 32\n",
        "drop_prob = 0.5"
      ],
      "metadata": {
        "id": "sX6B3sjC4L-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For FastText Embeddings"
      ],
      "metadata": {
        "id": "ybxQNXvd4StH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "weights_matrix = ft_weights_matrix \n",
        "output_size = 3\n",
        "hidden_dim = 64\n",
        "drop_prob = 0.5"
      ],
      "metadata": {
        "id": "3qdqXZDp4U1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORKS MODELS"
      ],
      "metadata": {
        "id": "-vSrrfArkszg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Network Architecture with LSTM"
      ],
      "metadata": {
        "id": "TPx2CVyk35N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Architecture 1"
      ],
      "metadata": {
        "id": "0GoFOYWy-wVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The RNN model that will be used to perform classification\n",
        "class RNN1(nn.Module):\n",
        "\n",
        "    def __init__(self, weights_matrix, output_size, hidden_dim, drop_prob):\n",
        "\n",
        "        super(RNN1, self).__init__()\n",
        "        \n",
        "        # embedding layers\n",
        "        self.embedding, self.num_embeddings, self.embeddings_size = create_emb_layer(weights_matrix, True)\n",
        "        # embedding dropout\n",
        "        self.dropout = nn.Dropout2d(drop_prob)\n",
        "        \n",
        "        # First lstm and GRU layers\n",
        "        self.lstm1 = nn.LSTM(self.embeddings_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru1 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # Second lstm and GRU layers\n",
        "        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # linear\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.out = nn.Linear(hidden_dim * 2, output_size)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        # embedding output\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = torch.squeeze(torch.unsqueeze(embeds, 0))\n",
        "        \n",
        "        # lstm, and gru outputs\n",
        "        lstm_out, _ = self.lstm1(embeds)\n",
        "        # slice lstm_out to just get output of last element of the input sequence\n",
        "#        lstm_out = lstm_out[:, -1]\n",
        "        gru_out, _ = self.gru1(lstm_out)\n",
        "        gru_out = gru_out.view(batch_size, -1, hidden_dim * 2)\n",
        "        lstm_out, _ = self.lstm2(gru_out)\n",
        "        # slice lstm_out to just get output of last element of the input sequence\n",
        "        lstm_out = lstm_out[:, -1]\n",
        "        gru_out, _ = self.gru2(lstm_out)\n",
        "        \n",
        "        # linear outputs\n",
        "        fc_out = self.softmax(gru_out)\n",
        "        final_out = self.out(fc_out)\n",
        "    \n",
        "        return final_out\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Configuration options\n",
        "  k_folds = 3\n",
        "  num_epochs = 2\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  \n",
        "  # For fold results\n",
        "  results = {}\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=42, shuffle=True,\n",
        "                                                            stratify=labels)\n",
        "  \n",
        "  dataset_train_part = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "  dataset_test_part = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
        "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
        "  \n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    \n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset, \n",
        "                      batch_size=10, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=test_subsampler)\n",
        "    \n",
        "    # Init the neural network and optimizer\n",
        "    network = RNN1(weights_matrix, output_size, hidden_dim, drop_prob)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "      # Print epoch\n",
        "      print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "      # Set current loss value\n",
        "      current_loss = 0\n",
        "\n",
        "      # Iterate over the DataLoader for training data\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "        \n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "    \n",
        "    # Saving the model\n",
        "    save_path = f'./rnn1-ft-model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        output_labels = outputs.argmax(dim=1).detach()\n",
        "        scores = precision_recall_fscore_support(targets, output_labels, average='weighted', labels=np.unique(output_labels))\n",
        "        \n",
        "      # Print scores\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "      print('Precision : ', scores[0], 'Recall : ', scores[1], 'F-score : ', scores[2])  \n",
        "\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  sum = 0.0\n",
        "  for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "  print(f'Average: {sum/len(results.items())} %')"
      ],
      "metadata": {
        "id": "A1fR9EKxA5vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c876b97-b851-418c-c272-c6aedd003526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.107\n",
            "Loss after mini-batch   100: 1.095\n",
            "Loss after mini-batch   150: 1.080\n",
            "Loss after mini-batch   200: 1.070\n",
            "Loss after mini-batch   250: 1.058\n",
            "Loss after mini-batch   300: 1.052\n",
            "Loss after mini-batch   350: 1.045\n",
            "Loss after mini-batch   400: 1.040\n",
            "Loss after mini-batch   450: 1.029\n",
            "Loss after mini-batch   500: 1.023\n",
            "Loss after mini-batch   550: 1.020\n",
            "Loss after mini-batch   600: 1.009\n",
            "Loss after mini-batch   650: 1.006\n",
            "Loss after mini-batch   700: 0.999\n",
            "Loss after mini-batch   750: 0.979\n",
            "Loss after mini-batch   800: 0.987\n",
            "Loss after mini-batch   850: 0.979\n",
            "Loss after mini-batch   900: 0.970\n",
            "Loss after mini-batch   950: 0.975\n",
            "Loss after mini-batch  1000: 0.960\n",
            "Loss after mini-batch  1050: 0.965\n",
            "Loss after mini-batch  1100: 0.951\n",
            "Loss after mini-batch  1150: 0.956\n",
            "Loss after mini-batch  1200: 0.954\n",
            "Loss after mini-batch  1250: 0.949\n",
            "Loss after mini-batch  1300: 0.917\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.913\n",
            "Loss after mini-batch   100: 0.919\n",
            "Loss after mini-batch   150: 0.911\n",
            "Loss after mini-batch   200: 0.930\n",
            "Loss after mini-batch   250: 0.911\n",
            "Loss after mini-batch   300: 0.883\n",
            "Loss after mini-batch   350: 0.895\n",
            "Loss after mini-batch   400: 0.873\n",
            "Loss after mini-batch   450: 0.878\n",
            "Loss after mini-batch   500: 0.884\n",
            "Loss after mini-batch   550: 0.872\n",
            "Loss after mini-batch   600: 0.880\n",
            "Loss after mini-batch   650: 0.896\n",
            "Loss after mini-batch   700: 0.876\n",
            "Loss after mini-batch   750: 0.862\n",
            "Loss after mini-batch   800: 0.867\n",
            "Loss after mini-batch   850: 0.864\n",
            "Loss after mini-batch   900: 0.868\n",
            "Loss after mini-batch   950: 0.850\n",
            "Loss after mini-batch  1000: 0.857\n",
            "Loss after mini-batch  1050: 0.828\n",
            "Loss after mini-batch  1100: 0.835\n",
            "Loss after mini-batch  1150: 0.829\n",
            "Loss after mini-batch  1200: 0.804\n",
            "Loss after mini-batch  1250: 0.840\n",
            "Loss after mini-batch  1300: 0.833\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 0: 75 %\n",
            "Precision :  0.8 Recall :  1.0 F-score :  0.888888888888889\n",
            "FOLD 1\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.073\n",
            "Loss after mini-batch   100: 1.055\n",
            "Loss after mini-batch   150: 1.047\n",
            "Loss after mini-batch   200: 1.035\n",
            "Loss after mini-batch   250: 1.026\n",
            "Loss after mini-batch   300: 1.017\n",
            "Loss after mini-batch   350: 1.008\n",
            "Loss after mini-batch   400: 1.007\n",
            "Loss after mini-batch   450: 0.997\n",
            "Loss after mini-batch   500: 0.984\n",
            "Loss after mini-batch   550: 0.985\n",
            "Loss after mini-batch   600: 0.971\n",
            "Loss after mini-batch   650: 0.973\n",
            "Loss after mini-batch   700: 0.966\n",
            "Loss after mini-batch   750: 0.962\n",
            "Loss after mini-batch   800: 0.948\n",
            "Loss after mini-batch   850: 0.955\n",
            "Loss after mini-batch   900: 0.935\n",
            "Loss after mini-batch   950: 0.936\n",
            "Loss after mini-batch  1000: 0.937\n",
            "Loss after mini-batch  1050: 0.938\n",
            "Loss after mini-batch  1100: 0.918\n",
            "Loss after mini-batch  1150: 0.910\n",
            "Loss after mini-batch  1200: 0.907\n",
            "Loss after mini-batch  1250: 0.890\n",
            "Loss after mini-batch  1300: 0.907\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.893\n",
            "Loss after mini-batch   100: 0.910\n",
            "Loss after mini-batch   150: 0.891\n",
            "Loss after mini-batch   200: 0.886\n",
            "Loss after mini-batch   250: 0.877\n",
            "Loss after mini-batch   300: 0.873\n",
            "Loss after mini-batch   350: 0.866\n",
            "Loss after mini-batch   400: 0.850\n",
            "Loss after mini-batch   450: 0.863\n",
            "Loss after mini-batch   500: 0.849\n",
            "Loss after mini-batch   550: 0.853\n",
            "Loss after mini-batch   600: 0.848\n",
            "Loss after mini-batch   650: 0.842\n",
            "Loss after mini-batch   700: 0.839\n",
            "Loss after mini-batch   750: 0.839\n",
            "Loss after mini-batch   800: 0.835\n",
            "Loss after mini-batch   850: 0.849\n",
            "Loss after mini-batch   900: 0.815\n",
            "Loss after mini-batch   950: 0.854\n",
            "Loss after mini-batch  1000: 0.816\n",
            "Loss after mini-batch  1050: 0.845\n",
            "Loss after mini-batch  1100: 0.820\n",
            "Loss after mini-batch  1150: 0.823\n",
            "Loss after mini-batch  1200: 0.777\n",
            "Loss after mini-batch  1250: 0.818\n",
            "Loss after mini-batch  1300: 0.800\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 1: 75 %\n",
            "Precision :  1.0 Recall :  1.0 F-score :  1.0\n",
            "FOLD 2\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.096\n",
            "Loss after mini-batch   100: 1.084\n",
            "Loss after mini-batch   150: 1.067\n",
            "Loss after mini-batch   200: 1.053\n",
            "Loss after mini-batch   250: 1.052\n",
            "Loss after mini-batch   300: 1.046\n",
            "Loss after mini-batch   350: 1.032\n",
            "Loss after mini-batch   400: 1.021\n",
            "Loss after mini-batch   450: 1.011\n",
            "Loss after mini-batch   500: 1.005\n",
            "Loss after mini-batch   550: 1.003\n",
            "Loss after mini-batch   600: 0.997\n",
            "Loss after mini-batch   650: 0.991\n",
            "Loss after mini-batch   700: 0.987\n",
            "Loss after mini-batch   750: 0.981\n",
            "Loss after mini-batch   800: 0.985\n",
            "Loss after mini-batch   850: 0.969\n",
            "Loss after mini-batch   900: 0.971\n",
            "Loss after mini-batch   950: 0.961\n",
            "Loss after mini-batch  1000: 0.946\n",
            "Loss after mini-batch  1050: 0.952\n",
            "Loss after mini-batch  1100: 0.946\n",
            "Loss after mini-batch  1150: 0.942\n",
            "Loss after mini-batch  1200: 0.912\n",
            "Loss after mini-batch  1250: 0.935\n",
            "Loss after mini-batch  1300: 0.924\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.904\n",
            "Loss after mini-batch   100: 0.917\n",
            "Loss after mini-batch   150: 0.908\n",
            "Loss after mini-batch   200: 0.885\n",
            "Loss after mini-batch   250: 0.898\n",
            "Loss after mini-batch   300: 0.888\n",
            "Loss after mini-batch   350: 0.901\n",
            "Loss after mini-batch   400: 0.865\n",
            "Loss after mini-batch   450: 0.869\n",
            "Loss after mini-batch   500: 0.892\n",
            "Loss after mini-batch   550: 0.896\n",
            "Loss after mini-batch   600: 0.854\n",
            "Loss after mini-batch   650: 0.853\n",
            "Loss after mini-batch   700: 0.871\n",
            "Loss after mini-batch   750: 0.837\n",
            "Loss after mini-batch   800: 0.845\n",
            "Loss after mini-batch   850: 0.861\n",
            "Loss after mini-batch   900: 0.839\n",
            "Loss after mini-batch   950: 0.836\n",
            "Loss after mini-batch  1000: 0.839\n",
            "Loss after mini-batch  1050: 0.845\n",
            "Loss after mini-batch  1100: 0.819\n",
            "Loss after mini-batch  1150: 0.834\n",
            "Loss after mini-batch  1200: 0.830\n",
            "Loss after mini-batch  1250: 0.801\n",
            "Loss after mini-batch  1300: 0.856\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 2: 75 %\n",
            "Precision :  0.5 Recall :  1.0 F-score :  0.6666666666666666\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 3 FOLDS\n",
            "Fold 0: 75.99702380952381 %\n",
            "Fold 1: 75.29761904761905 %\n",
            "Fold 2: 75.35714285714286 %\n",
            "Average: 75.55059523809524 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Architecture 2"
      ],
      "metadata": {
        "id": "_cQE9mMf-tlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN2(nn.Module):\n",
        "\n",
        "    def __init__(self, weights_matrix, output_size, hidden_dim, drop_prob):\n",
        "\n",
        "        super(RNN2, self).__init__()\n",
        "        \n",
        "        # embedding layers\n",
        "        self.embedding, self.num_embeddings, self.embeddings_size = create_emb_layer(weights_matrix, True)\n",
        "        \n",
        "        # embedding dropout\n",
        "        self.dropout = nn.Dropout2d(drop_prob)\n",
        "        \n",
        "        # First lstm and GRU layers\n",
        "        self.lstm1 = nn.LSTM(self.embeddings_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru1 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # Second lstm and GRU layers\n",
        "        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Third lstm and GRU layers\n",
        "        self.lstm3 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru3 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # linear\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.out = nn.Linear(hidden_dim * 2, output_size)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        # embedding output\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = torch.squeeze(torch.unsqueeze(embeds, 0))\n",
        "        \n",
        "        # lstm, and gru outputs\n",
        "        lstm_out, _ = self.lstm1(embeds)\n",
        "        # slice lstm_out to just get output of last element of the input sequence\n",
        "        lstm_out = lstm_out[:, -1]\n",
        "        gru_out, _ = self.gru1(lstm_out)\n",
        "        gru_out = gru_out.view(batch_size, -1, hidden_dim * 2)\n",
        "        lstm_out, _ = self.lstm2(gru_out)\n",
        "        # slice lstm_out to just get output of last element of the input sequence\n",
        "        lstm_out = lstm_out[:, -1]\n",
        "        gru_out, _ = self.gru2(lstm_out)\n",
        "        lstm_out, _ = self.lstm3(gru_out)\n",
        "        # slice lstm_out to just get output of last element of the input sequence\n",
        "  #      lstm_out = lstm_out[:, -2]\n",
        "        gru_out, _ = self.gru3(lstm_out)\n",
        "        # linear outputs\n",
        "        fc_out = self.softmax(gru_out)\n",
        "        final_out = self.out(fc_out)\n",
        "    \n",
        "        return final_out\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Configuration options\n",
        "  k_folds = 3\n",
        "  num_epochs = 2\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  \n",
        "  # For fold results\n",
        "  results = {}\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=42, shuffle=True,\n",
        "                                                            stratify=labels)\n",
        "  \n",
        "  dataset_train_part = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "  dataset_test_part = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
        "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
        "  \n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    \n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset, \n",
        "                      batch_size=10, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=test_subsampler)\n",
        "    \n",
        "    # Init the neural network and optimizer\n",
        "    network = RNN2(weights_matrix, output_size, hidden_dim, drop_prob)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "      # Print epoch\n",
        "      print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "      # Set current loss value\n",
        "      current_loss = 0\n",
        "\n",
        "      # Iterate over the DataLoader for training data\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "        \n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "    \n",
        "    # Saving the model\n",
        "    save_path = f'./rnn2-ft-model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        output_labels = outputs.argmax(dim=1).detach()\n",
        "        scores = precision_recall_fscore_support(targets, output_labels, average='weighted', labels=np.unique(output_labels))\n",
        "        \n",
        "      # Print scores\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "      print('Precision : ', scores[0], 'Recall : ', scores[1], 'F-score : ', scores[2])  \n",
        "\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  sum = 0.0\n",
        "  for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "  print(f'Average: {sum/len(results.items())} %')"
      ],
      "metadata": {
        "id": "S9ThxBKF-trS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d1875d-736f-4504-d89b-c3ac6697f5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.091\n",
            "Loss after mini-batch   100: 1.077\n",
            "Loss after mini-batch   150: 1.059\n",
            "Loss after mini-batch   200: 1.046\n",
            "Loss after mini-batch   250: 1.043\n",
            "Loss after mini-batch   300: 1.036\n",
            "Loss after mini-batch   350: 1.027\n",
            "Loss after mini-batch   400: 1.017\n",
            "Loss after mini-batch   450: 1.014\n",
            "Loss after mini-batch   500: 1.008\n",
            "Loss after mini-batch   550: 1.001\n",
            "Loss after mini-batch   600: 0.998\n",
            "Loss after mini-batch   650: 0.990\n",
            "Loss after mini-batch   700: 0.974\n",
            "Loss after mini-batch   750: 0.977\n",
            "Loss after mini-batch   800: 0.968\n",
            "Loss after mini-batch   850: 0.962\n",
            "Loss after mini-batch   900: 0.960\n",
            "Loss after mini-batch   950: 0.968\n",
            "Loss after mini-batch  1000: 0.948\n",
            "Loss after mini-batch  1050: 0.950\n",
            "Loss after mini-batch  1100: 0.928\n",
            "Loss after mini-batch  1150: 0.918\n",
            "Loss after mini-batch  1200: 0.918\n",
            "Loss after mini-batch  1250: 0.930\n",
            "Loss after mini-batch  1300: 0.916\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.902\n",
            "Loss after mini-batch   100: 0.905\n",
            "Loss after mini-batch   150: 0.900\n",
            "Loss after mini-batch   200: 0.913\n",
            "Loss after mini-batch   250: 0.889\n",
            "Loss after mini-batch   300: 0.883\n",
            "Loss after mini-batch   350: 0.892\n",
            "Loss after mini-batch   400: 0.890\n",
            "Loss after mini-batch   450: 0.872\n",
            "Loss after mini-batch   500: 0.858\n",
            "Loss after mini-batch   550: 0.862\n",
            "Loss after mini-batch   600: 0.866\n",
            "Loss after mini-batch   650: 0.860\n",
            "Loss after mini-batch   700: 0.833\n",
            "Loss after mini-batch   750: 0.849\n",
            "Loss after mini-batch   800: 0.819\n",
            "Loss after mini-batch   850: 0.852\n",
            "Loss after mini-batch   900: 0.860\n",
            "Loss after mini-batch   950: 0.852\n",
            "Loss after mini-batch  1000: 0.810\n",
            "Loss after mini-batch  1050: 0.835\n",
            "Loss after mini-batch  1100: 0.852\n",
            "Loss after mini-batch  1150: 0.795\n",
            "Loss after mini-batch  1200: 0.809\n",
            "Loss after mini-batch  1250: 0.832\n",
            "Loss after mini-batch  1300: 0.817\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 0: 75 %\n",
            "Precision :  0.8 Recall :  1.0 F-score :  0.888888888888889\n",
            "FOLD 1\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.075\n",
            "Loss after mini-batch   100: 1.064\n",
            "Loss after mini-batch   150: 1.048\n",
            "Loss after mini-batch   200: 1.031\n",
            "Loss after mini-batch   250: 1.025\n",
            "Loss after mini-batch   300: 1.015\n",
            "Loss after mini-batch   350: 1.017\n",
            "Loss after mini-batch   400: 0.999\n",
            "Loss after mini-batch   450: 1.005\n",
            "Loss after mini-batch   500: 0.990\n",
            "Loss after mini-batch   550: 0.989\n",
            "Loss after mini-batch   600: 0.980\n",
            "Loss after mini-batch   650: 0.975\n",
            "Loss after mini-batch   700: 0.968\n",
            "Loss after mini-batch   750: 0.960\n",
            "Loss after mini-batch   800: 0.958\n",
            "Loss after mini-batch   850: 0.949\n",
            "Loss after mini-batch   900: 0.939\n",
            "Loss after mini-batch   950: 0.941\n",
            "Loss after mini-batch  1000: 0.932\n",
            "Loss after mini-batch  1050: 0.930\n",
            "Loss after mini-batch  1100: 0.930\n",
            "Loss after mini-batch  1150: 0.933\n",
            "Loss after mini-batch  1200: 0.906\n",
            "Loss after mini-batch  1250: 0.919\n",
            "Loss after mini-batch  1300: 0.914\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.898\n",
            "Loss after mini-batch   100: 0.883\n",
            "Loss after mini-batch   150: 0.875\n",
            "Loss after mini-batch   200: 0.883\n",
            "Loss after mini-batch   250: 0.882\n",
            "Loss after mini-batch   300: 0.871\n",
            "Loss after mini-batch   350: 0.874\n",
            "Loss after mini-batch   400: 0.861\n",
            "Loss after mini-batch   450: 0.870\n",
            "Loss after mini-batch   500: 0.846\n",
            "Loss after mini-batch   550: 0.850\n",
            "Loss after mini-batch   600: 0.890\n",
            "Loss after mini-batch   650: 0.856\n",
            "Loss after mini-batch   700: 0.859\n",
            "Loss after mini-batch   750: 0.831\n",
            "Loss after mini-batch   800: 0.864\n",
            "Loss after mini-batch   850: 0.844\n",
            "Loss after mini-batch   900: 0.818\n",
            "Loss after mini-batch   950: 0.813\n",
            "Loss after mini-batch  1000: 0.824\n",
            "Loss after mini-batch  1050: 0.814\n",
            "Loss after mini-batch  1100: 0.833\n",
            "Loss after mini-batch  1150: 0.800\n",
            "Loss after mini-batch  1200: 0.812\n",
            "Loss after mini-batch  1250: 0.831\n",
            "Loss after mini-batch  1300: 0.812\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 1: 75 %\n",
            "Precision :  0.7 Recall :  1.0 F-score :  0.8235294117647058\n",
            "FOLD 2\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.084\n",
            "Loss after mini-batch   100: 1.068\n",
            "Loss after mini-batch   150: 1.047\n",
            "Loss after mini-batch   200: 1.036\n",
            "Loss after mini-batch   250: 1.028\n",
            "Loss after mini-batch   300: 1.025\n",
            "Loss after mini-batch   350: 1.023\n",
            "Loss after mini-batch   400: 1.013\n",
            "Loss after mini-batch   450: 1.008\n",
            "Loss after mini-batch   500: 0.993\n",
            "Loss after mini-batch   550: 0.995\n",
            "Loss after mini-batch   600: 0.986\n",
            "Loss after mini-batch   650: 0.977\n",
            "Loss after mini-batch   700: 0.981\n",
            "Loss after mini-batch   750: 0.968\n",
            "Loss after mini-batch   800: 0.968\n",
            "Loss after mini-batch   850: 0.962\n",
            "Loss after mini-batch   900: 0.945\n",
            "Loss after mini-batch   950: 0.941\n",
            "Loss after mini-batch  1000: 0.952\n",
            "Loss after mini-batch  1050: 0.927\n",
            "Loss after mini-batch  1100: 0.935\n",
            "Loss after mini-batch  1150: 0.928\n",
            "Loss after mini-batch  1200: 0.929\n",
            "Loss after mini-batch  1250: 0.919\n",
            "Loss after mini-batch  1300: 0.907\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.919\n",
            "Loss after mini-batch   100: 0.891\n",
            "Loss after mini-batch   150: 0.887\n",
            "Loss after mini-batch   200: 0.890\n",
            "Loss after mini-batch   250: 0.885\n",
            "Loss after mini-batch   300: 0.899\n",
            "Loss after mini-batch   350: 0.886\n",
            "Loss after mini-batch   400: 0.868\n",
            "Loss after mini-batch   450: 0.890\n",
            "Loss after mini-batch   500: 0.853\n",
            "Loss after mini-batch   550: 0.870\n",
            "Loss after mini-batch   600: 0.840\n",
            "Loss after mini-batch   650: 0.831\n",
            "Loss after mini-batch   700: 0.868\n",
            "Loss after mini-batch   750: 0.858\n",
            "Loss after mini-batch   800: 0.836\n",
            "Loss after mini-batch   850: 0.828\n",
            "Loss after mini-batch   900: 0.844\n",
            "Loss after mini-batch   950: 0.826\n",
            "Loss after mini-batch  1000: 0.838\n",
            "Loss after mini-batch  1050: 0.825\n",
            "Loss after mini-batch  1100: 0.832\n",
            "Loss after mini-batch  1150: 0.844\n",
            "Loss after mini-batch  1200: 0.804\n",
            "Loss after mini-batch  1250: 0.787\n",
            "Loss after mini-batch  1300: 0.818\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 2: 75 %\n",
            "Precision :  0.5 Recall :  1.0 F-score :  0.6666666666666666\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 3 FOLDS\n",
            "Fold 0: 75.46130952380953 %\n",
            "Fold 1: 75.61011904761905 %\n",
            "Fold 2: 75.58035714285715 %\n",
            "Average: 75.55059523809524 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs)"
      ],
      "metadata": {
        "id": "v9ot7GRdott3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "5rz5NPWnqiPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Word2Vec Embeddings"
      ],
      "metadata": {
        "id": "ohx_XcJUqjo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "weights_matrix = word2vec_weights_matrix\n",
        "num_classes = 3\n",
        "hidden_dim = 64\n",
        "drop_prob = 0.5\n",
        "padding_idx = 1"
      ],
      "metadata": {
        "id": "97T63NXXqmFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Glove Embeddings"
      ],
      "metadata": {
        "id": "o6qAcLEAq2zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "weights_matrix = glove_weights_matrix \n",
        "output_size = 3\n",
        "hidden_dim = 32\n",
        "drop_prob = 0.5\n",
        "padding_idx = 1"
      ],
      "metadata": {
        "id": "TmEEJr1sq4EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For FastText Embeddings"
      ],
      "metadata": {
        "id": "fre4bDutq6Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters \n",
        "weights_matrix = ft_weights_matrix\n",
        "num_classes = 3\n",
        "hidden_dim = 128\n",
        "drop_prob = 0.5\n",
        "padding_idx = 1"
      ],
      "metadata": {
        "id": "6RMs0KUkq9WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architecture 1"
      ],
      "metadata": {
        "id": "zkZBjhzK_tMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1(nn.Module):\n",
        "    def __init__(self, weights_matrix, drop_prob, padding_idx, num_classes):\n",
        "        super(CNN1, self).__init__()\n",
        "\n",
        "        self.embedding, self.num_embeddings, self.embeddings_size = create_emb_layer(weights_matrix, True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # conv2d\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=256,\n",
        "                               kernel_size=(3, self.embeddings_size),\n",
        "                               stride=1) # output: [batch_size, output_channels, max_len - 3 + 1, 1]\n",
        "        self.conv2 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=128,\n",
        "                               kernel_size=(3, 256),\n",
        "                               stride=1) # output: [batch_size, output_channels, , 1]\n",
        "\n",
        "        # linear\n",
        "        self.fc1 = nn.Linear(128, 32)\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "        # log soft max\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        embedded = self.embedding(inputs) # [batch_size, len, embedding_size]\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # conv\n",
        "        conv1_output = self.conv1(embedded.unsqueeze(1))\n",
        "        conv1_output = self.softmax (conv1_output)\n",
        "        conv1_output = conv1_output.squeeze(3).transpose(1, 2) # [batch_size, len - stride + 1, output_channels]\n",
        "\n",
        "        conv2_output = self.conv2(conv1_output.unsqueeze(1))\n",
        "        conv2_output = self.softmax (conv2_output)\n",
        "\n",
        "        # max pool [batch_size, output_channels, 1, 1]\n",
        "        max_pool_output = F.max_pool2d(conv2_output, kernel_size=(conv2_output.shape[2], 1))\n",
        "\n",
        "        # [batch_size, output_channels] out_channels\n",
        "        fc1_input = max_pool_output.squeeze() #[batch_size, output_channels]\n",
        "\n",
        "        fc1_output = self.fc1(fc1_input)\n",
        "        fc2_output = self.fc2(fc1_output)\n",
        "\n",
        "        # softmax\n",
        "        output = self.softmax (fc2_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Configuration options\n",
        "  k_folds = 3\n",
        "  num_epochs = 2\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  \n",
        "  # For fold results\n",
        "  results = {}\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=42, shuffle=True,\n",
        "                                                            stratify=labels)\n",
        "  \n",
        "  dataset_train_part = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "  dataset_test_part = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
        "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
        "  \n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    \n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset, \n",
        "                      batch_size=10, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=test_subsampler)\n",
        "    \n",
        "    # Init the neural network and optimizer\n",
        "    network = CNN1(weights_matrix, drop_prob, padding_idx, num_classes)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "      # Print epoch\n",
        "      print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "      # Set current loss value\n",
        "      current_loss = 0\n",
        "\n",
        "      # Iterate over the DataLoader for training data\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "        \n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "    \n",
        "    # Saving the model\n",
        "    save_path = f'./cnn1-ft-model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        output_labels = outputs.argmax(dim=1).detach()\n",
        "        scores = precision_recall_fscore_support(targets, output_labels, average='weighted', labels=np.unique(output_labels))\n",
        "        \n",
        "      # Print scores\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "      print('Precision : ', scores[0], 'Recall : ', scores[1], 'F-score : ', scores[2])  \n",
        "\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  sum = 0.0\n",
        "  for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "  print(f'Average: {sum/len(results.items())} %')"
      ],
      "metadata": {
        "id": "MNVNXC4Uohbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd14ed7a-e631-46d9-fcf9-071066b2623d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.070\n",
            "Loss after mini-batch   100: 1.061\n",
            "Loss after mini-batch   150: 1.048\n",
            "Loss after mini-batch   200: 1.021\n",
            "Loss after mini-batch   250: 1.006\n",
            "Loss after mini-batch   300: 0.993\n",
            "Loss after mini-batch   350: 0.970\n",
            "Loss after mini-batch   400: 0.935\n",
            "Loss after mini-batch   450: 0.932\n",
            "Loss after mini-batch   500: 0.918\n",
            "Loss after mini-batch   550: 0.875\n",
            "Loss after mini-batch   600: 0.878\n",
            "Loss after mini-batch   650: 0.854\n",
            "Loss after mini-batch   700: 0.872\n",
            "Loss after mini-batch   750: 0.867\n",
            "Loss after mini-batch   800: 0.854\n",
            "Loss after mini-batch   850: 0.825\n",
            "Loss after mini-batch   900: 0.816\n",
            "Loss after mini-batch   950: 0.778\n",
            "Loss after mini-batch  1000: 0.796\n",
            "Loss after mini-batch  1050: 0.800\n",
            "Loss after mini-batch  1100: 0.831\n",
            "Loss after mini-batch  1150: 0.804\n",
            "Loss after mini-batch  1200: 0.798\n",
            "Loss after mini-batch  1250: 0.823\n",
            "Loss after mini-batch  1300: 0.805\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.823\n",
            "Loss after mini-batch   100: 0.813\n",
            "Loss after mini-batch   150: 0.823\n",
            "Loss after mini-batch   200: 0.789\n",
            "Loss after mini-batch   250: 0.809\n",
            "Loss after mini-batch   300: 0.779\n",
            "Loss after mini-batch   350: 0.793\n",
            "Loss after mini-batch   400: 0.794\n",
            "Loss after mini-batch   450: 0.796\n",
            "Loss after mini-batch   500: 0.797\n",
            "Loss after mini-batch   550: 0.791\n",
            "Loss after mini-batch   600: 0.773\n",
            "Loss after mini-batch   650: 0.769\n",
            "Loss after mini-batch   700: 0.806\n",
            "Loss after mini-batch   750: 0.804\n",
            "Loss after mini-batch   800: 0.800\n",
            "Loss after mini-batch   850: 0.800\n",
            "Loss after mini-batch   900: 0.802\n",
            "Loss after mini-batch   950: 0.784\n",
            "Loss after mini-batch  1000: 0.811\n",
            "Loss after mini-batch  1050: 0.795\n",
            "Loss after mini-batch  1100: 0.817\n",
            "Loss after mini-batch  1150: 0.787\n",
            "Loss after mini-batch  1200: 0.799\n",
            "Loss after mini-batch  1250: 0.813\n",
            "Loss after mini-batch  1300: 0.831\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 0: 75 %\n",
            "Precision :  0.6 Recall :  1.0 F-score :  0.7499999999999999\n",
            "FOLD 1\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.066\n",
            "Loss after mini-batch   100: 1.053\n",
            "Loss after mini-batch   150: 1.044\n",
            "Loss after mini-batch   200: 1.026\n",
            "Loss after mini-batch   250: 1.011\n",
            "Loss after mini-batch   300: 0.995\n",
            "Loss after mini-batch   350: 0.965\n",
            "Loss after mini-batch   400: 0.942\n",
            "Loss after mini-batch   450: 0.939\n",
            "Loss after mini-batch   500: 0.927\n",
            "Loss after mini-batch   550: 0.889\n",
            "Loss after mini-batch   600: 0.899\n",
            "Loss after mini-batch   650: 0.855\n",
            "Loss after mini-batch   700: 0.852\n",
            "Loss after mini-batch   750: 0.847\n",
            "Loss after mini-batch   800: 0.831\n",
            "Loss after mini-batch   850: 0.842\n",
            "Loss after mini-batch   900: 0.806\n",
            "Loss after mini-batch   950: 0.818\n",
            "Loss after mini-batch  1000: 0.833\n",
            "Loss after mini-batch  1050: 0.830\n",
            "Loss after mini-batch  1100: 0.794\n",
            "Loss after mini-batch  1150: 0.806\n",
            "Loss after mini-batch  1200: 0.814\n",
            "Loss after mini-batch  1250: 0.859\n",
            "Loss after mini-batch  1300: 0.806\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.793\n",
            "Loss after mini-batch   100: 0.765\n",
            "Loss after mini-batch   150: 0.816\n",
            "Loss after mini-batch   200: 0.788\n",
            "Loss after mini-batch   250: 0.768\n",
            "Loss after mini-batch   300: 0.826\n",
            "Loss after mini-batch   350: 0.808\n",
            "Loss after mini-batch   400: 0.780\n",
            "Loss after mini-batch   450: 0.843\n",
            "Loss after mini-batch   500: 0.786\n",
            "Loss after mini-batch   550: 0.786\n",
            "Loss after mini-batch   600: 0.824\n",
            "Loss after mini-batch   650: 0.858\n",
            "Loss after mini-batch   700: 0.793\n",
            "Loss after mini-batch   750: 0.777\n",
            "Loss after mini-batch   800: 0.810\n",
            "Loss after mini-batch   850: 0.828\n",
            "Loss after mini-batch   900: 0.804\n",
            "Loss after mini-batch   950: 0.823\n",
            "Loss after mini-batch  1000: 0.776\n",
            "Loss after mini-batch  1050: 0.819\n",
            "Loss after mini-batch  1100: 0.760\n",
            "Loss after mini-batch  1150: 0.819\n",
            "Loss after mini-batch  1200: 0.789\n",
            "Loss after mini-batch  1250: 0.787\n",
            "Loss after mini-batch  1300: 0.793\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 1: 75 %\n",
            "Precision :  0.6 Recall :  1.0 F-score :  0.7499999999999999\n",
            "FOLD 2\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.100\n",
            "Loss after mini-batch   100: 1.086\n",
            "Loss after mini-batch   150: 1.075\n",
            "Loss after mini-batch   200: 1.057\n",
            "Loss after mini-batch   250: 1.041\n",
            "Loss after mini-batch   300: 1.018\n",
            "Loss after mini-batch   350: 1.001\n",
            "Loss after mini-batch   400: 0.977\n",
            "Loss after mini-batch   450: 0.935\n",
            "Loss after mini-batch   500: 0.915\n",
            "Loss after mini-batch   550: 0.904\n",
            "Loss after mini-batch   600: 0.881\n",
            "Loss after mini-batch   650: 0.874\n",
            "Loss after mini-batch   700: 0.864\n",
            "Loss after mini-batch   750: 0.848\n",
            "Loss after mini-batch   800: 0.827\n",
            "Loss after mini-batch   850: 0.818\n",
            "Loss after mini-batch   900: 0.831\n",
            "Loss after mini-batch   950: 0.843\n",
            "Loss after mini-batch  1000: 0.796\n",
            "Loss after mini-batch  1050: 0.808\n",
            "Loss after mini-batch  1100: 0.807\n",
            "Loss after mini-batch  1150: 0.820\n",
            "Loss after mini-batch  1200: 0.796\n",
            "Loss after mini-batch  1250: 0.830\n",
            "Loss after mini-batch  1300: 0.831\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.790\n",
            "Loss after mini-batch   100: 0.777\n",
            "Loss after mini-batch   150: 0.792\n",
            "Loss after mini-batch   200: 0.775\n",
            "Loss after mini-batch   250: 0.806\n",
            "Loss after mini-batch   300: 0.791\n",
            "Loss after mini-batch   350: 0.804\n",
            "Loss after mini-batch   400: 0.798\n",
            "Loss after mini-batch   450: 0.829\n",
            "Loss after mini-batch   500: 0.788\n",
            "Loss after mini-batch   550: 0.782\n",
            "Loss after mini-batch   600: 0.779\n",
            "Loss after mini-batch   650: 0.789\n",
            "Loss after mini-batch   700: 0.789\n",
            "Loss after mini-batch   750: 0.792\n",
            "Loss after mini-batch   800: 0.804\n",
            "Loss after mini-batch   850: 0.820\n",
            "Loss after mini-batch   900: 0.820\n",
            "Loss after mini-batch   950: 0.800\n",
            "Loss after mini-batch  1000: 0.800\n",
            "Loss after mini-batch  1050: 0.798\n",
            "Loss after mini-batch  1100: 0.815\n",
            "Loss after mini-batch  1150: 0.809\n",
            "Loss after mini-batch  1200: 0.799\n",
            "Loss after mini-batch  1250: 0.817\n",
            "Loss after mini-batch  1300: 0.823\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 2: 75 %\n",
            "Precision :  0.7 Recall :  1.0 F-score :  0.8235294117647058\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 3 FOLDS\n",
            "Fold 0: 75.625 %\n",
            "Fold 1: 75.78869047619048 %\n",
            "Fold 2: 75.23809523809524 %\n",
            "Average: 75.55059523809524 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architecture 2"
      ],
      "metadata": {
        "id": "qcI6-eFh_v7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN2(nn.Module):\n",
        "    def __init__(self, weights_matrix, drop_prob, padding_idx, num_classes):\n",
        "        super(CNN2, self).__init__()\n",
        "\n",
        "        self.embedding, self.num_embeddings, self.embeddings_size = create_emb_layer(weights_matrix, True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # conv2d\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=256,\n",
        "                               kernel_size=(3, self.embeddings_size),\n",
        "                               stride=1) # output: [batch_size, output_channels, max_len - 3 + 1, 1]\n",
        "        self.conv2 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=128,\n",
        "                               kernel_size=(3, 256),\n",
        "                               stride=1) # output: [batch_size, output_channels, , 1]\n",
        "        self.conv3 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=64,\n",
        "                               kernel_size=(3, 128),\n",
        "                               stride=1) \n",
        "        \n",
        "        # linear\n",
        "        self.fc1 = nn.Linear(128, 32)\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "        # log soft max\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        embedded = self.embedding(inputs) # [batch_size, len, embedding_size]\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # conv\n",
        "        conv1_output = self.conv1(embedded.unsqueeze(1))\n",
        "        conv1_output =  self.softmax(conv1_output)\n",
        "        conv1_output = conv1_output.squeeze(3).transpose(1, 2) # [batch_size, len - stride + 1, output_channels]\n",
        "\n",
        "        conv2_output = self.conv2(conv1_output.unsqueeze(1))\n",
        "        conv2_output =  self.softmax(conv2_output)\n",
        "\n",
        "        # max pool [batch_size, output_channels, 1, 1]\n",
        "        max_pool_output = F.max_pool2d(conv2_output, kernel_size=(conv2_output.shape[2], 1))\n",
        "\n",
        "        # [batch_size, output_channels] out_channels\n",
        "        fc1_input = max_pool_output.squeeze() #[batch_size, output_channels]\n",
        "\n",
        "        fc1_output = self.fc1(fc1_input)\n",
        "        fc2_output = self.fc2(fc1_output)\n",
        "\n",
        "        # softmax\n",
        "        output =  self.softmax(fc2_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Configuration options\n",
        "  k_folds = 3\n",
        "  num_epochs = 2\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  \n",
        "  # For fold results\n",
        "  results = {}\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=42, shuffle=True,\n",
        "                                                            stratify=labels)\n",
        "  \n",
        "  dataset_train_part = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "  dataset_test_part = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
        "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
        "  \n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    \n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset, \n",
        "                      batch_size=10, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=test_subsampler)\n",
        "    \n",
        "    # Init the neural network and optimizer\n",
        "    network = CNN2(weights_matrix, drop_prob, padding_idx, num_classes)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "      # Print epoch\n",
        "      print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "      # Set current loss value\n",
        "      current_loss = 0\n",
        "\n",
        "      # Iterate over the DataLoader for training data\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "        \n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "    \n",
        "    # Saving the model\n",
        "    save_path = f'./cnn-ft-model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        output_labels = outputs.argmax(dim=1).detach()\n",
        "        scores = precision_recall_fscore_support(targets, output_labels, average='weighted', labels=np.unique(output_labels))\n",
        "        \n",
        "      # Print scores\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "      print('Precision : ', scores[0], 'Recall : ', scores[1], 'F-score : ', scores[2])  \n",
        "\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  sum = 0.0\n",
        "  for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "  print(f'Average: {sum/len(results.items())} %')"
      ],
      "metadata": {
        "id": "PoZVDPmo_fnf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6444a9c9-8bcf-44e1-ec38-a9f91596f02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-ce99bd035e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-ce99bd035e5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mfc1_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[batch_size, output_channels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mfc1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc1_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mfc2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc1_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2560x257 and 128x32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN 3"
      ],
      "metadata": {
        "id": "coNs3KTuCoNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1(nn.Module):\n",
        "    def __init__(self, weights_matrix, drop_prob, padding_idx, num_classes):\n",
        "        super(CNN1, self).__init__()\n",
        "\n",
        "        self.embedding, self.num_embeddings, self.embeddings_size = create_emb_layer(weights_matrix, True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # conv2d\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=256,\n",
        "                               kernel_size=(3, self.embeddings_size),\n",
        "                               stride=2) # output: [batch_size, output_channels, max_len - 3 + 1, 1]\n",
        "        self.conv2 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=128,\n",
        "                               kernel_size=(3, 256),\n",
        "                               stride=2) # output: [batch_size, output_channels, , 1]\n",
        "\n",
        "        # linear\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "        # log soft max\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        embedded = self.embedding(inputs) # [batch_size, len, embedding_size]\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # conv\n",
        "        conv1_output = self.conv1(embedded.unsqueeze(1))\n",
        "        conv1_output = self.softmax (conv1_output)\n",
        "        conv1_output = conv1_output.squeeze(3).transpose(1, 2) # [batch_size, len - stride + 1, output_channels]\n",
        "\n",
        "        conv2_output = self.conv2(conv1_output.unsqueeze(1))\n",
        "        conv2_output = self.softmax (conv2_output)\n",
        "\n",
        "        # max pool [batch_size, output_channels, 1, 1]\n",
        "        max_pool_output = F.max_pool2d(conv2_output, kernel_size=(conv2_output.shape[2], 1))\n",
        "\n",
        "        # [batch_size, output_channels] out_channels\n",
        "        fc1_input = max_pool_output.squeeze() #[batch_size, output_channels]\n",
        "\n",
        "        fc1_output = self.fc1(fc1_input)\n",
        "        fc2_output = self.fc2(fc1_output)\n",
        "\n",
        "        # softmax\n",
        "        output = self.softmax (fc2_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Configuration options\n",
        "  k_folds = 3\n",
        "  num_epochs = 2\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  \n",
        "  # For fold results\n",
        "  results = {}\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=42, shuffle=True,\n",
        "                                                            stratify=labels)\n",
        "  \n",
        "  dataset_train_part = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "  dataset_test_part = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
        "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
        "  \n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    \n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset, \n",
        "                      batch_size=10, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=10, sampler=test_subsampler)\n",
        "    \n",
        "    # Init the neural network and optimizer\n",
        "    network = CNN1(weights_matrix, drop_prob, padding_idx, num_classes)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "    \n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "      # Print epoch\n",
        "      print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "      # Set current loss value\n",
        "      current_loss = 0\n",
        "\n",
        "      # Iterate over the DataLoader for training data\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "        \n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "    \n",
        "    # Saving the model\n",
        "    save_path = f'./cnn3-ft-model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        output_labels = outputs.argmax(dim=1).detach()\n",
        "        scores = precision_recall_fscore_support(targets, output_labels, average='weighted', labels=np.unique(output_labels))\n",
        "        \n",
        "      # Print scores\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "      print('Precision : ', scores[0], 'Recall : ', scores[1], 'F-score : ', scores[2])  \n",
        "\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  sum = 0.0\n",
        "  for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "  print(f'Average: {sum/len(results.items())} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPYdTvgJCnqc",
        "outputId": "bdb626de-b2b2-41a9-a385-77b9ce54d7ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.074\n",
            "Loss after mini-batch   100: 1.057\n",
            "Loss after mini-batch   150: 1.040\n",
            "Loss after mini-batch   200: 1.024\n",
            "Loss after mini-batch   250: 1.009\n",
            "Loss after mini-batch   300: 0.972\n",
            "Loss after mini-batch   350: 0.961\n",
            "Loss after mini-batch   400: 0.939\n",
            "Loss after mini-batch   450: 0.937\n",
            "Loss after mini-batch   500: 0.902\n",
            "Loss after mini-batch   550: 0.899\n",
            "Loss after mini-batch   600: 0.874\n",
            "Loss after mini-batch   650: 0.849\n",
            "Loss after mini-batch   700: 0.831\n",
            "Loss after mini-batch   750: 0.835\n",
            "Loss after mini-batch   800: 0.838\n",
            "Loss after mini-batch   850: 0.828\n",
            "Loss after mini-batch   900: 0.799\n",
            "Loss after mini-batch   950: 0.812\n",
            "Loss after mini-batch  1000: 0.845\n",
            "Loss after mini-batch  1050: 0.800\n",
            "Loss after mini-batch  1100: 0.833\n",
            "Loss after mini-batch  1150: 0.817\n",
            "Loss after mini-batch  1200: 0.821\n",
            "Loss after mini-batch  1250: 0.814\n",
            "Loss after mini-batch  1300: 0.798\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.781\n",
            "Loss after mini-batch   100: 0.791\n",
            "Loss after mini-batch   150: 0.795\n",
            "Loss after mini-batch   200: 0.786\n",
            "Loss after mini-batch   250: 0.797\n",
            "Loss after mini-batch   300: 0.814\n",
            "Loss after mini-batch   350: 0.820\n",
            "Loss after mini-batch   400: 0.804\n",
            "Loss after mini-batch   450: 0.800\n",
            "Loss after mini-batch   500: 0.807\n",
            "Loss after mini-batch   550: 0.797\n",
            "Loss after mini-batch   600: 0.799\n",
            "Loss after mini-batch   650: 0.781\n",
            "Loss after mini-batch   700: 0.808\n",
            "Loss after mini-batch   750: 0.812\n",
            "Loss after mini-batch   800: 0.786\n",
            "Loss after mini-batch   850: 0.804\n",
            "Loss after mini-batch   900: 0.798\n",
            "Loss after mini-batch   950: 0.784\n",
            "Loss after mini-batch  1000: 0.829\n",
            "Loss after mini-batch  1050: 0.809\n",
            "Loss after mini-batch  1100: 0.776\n",
            "Loss after mini-batch  1150: 0.791\n",
            "Loss after mini-batch  1200: 0.797\n",
            "Loss after mini-batch  1250: 0.779\n",
            "Loss after mini-batch  1300: 0.797\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 0: 75 %\n",
            "Precision :  0.8999999999999999 Recall :  1.0 F-score :  0.9473684210526316\n",
            "FOLD 1\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.068\n",
            "Loss after mini-batch   100: 1.057\n",
            "Loss after mini-batch   150: 1.041\n",
            "Loss after mini-batch   200: 1.027\n",
            "Loss after mini-batch   250: 1.007\n",
            "Loss after mini-batch   300: 0.991\n",
            "Loss after mini-batch   350: 0.971\n",
            "Loss after mini-batch   400: 0.947\n",
            "Loss after mini-batch   450: 0.908\n",
            "Loss after mini-batch   500: 0.909\n",
            "Loss after mini-batch   550: 0.888\n",
            "Loss after mini-batch   600: 0.875\n",
            "Loss after mini-batch   650: 0.855\n",
            "Loss after mini-batch   700: 0.857\n",
            "Loss after mini-batch   750: 0.838\n",
            "Loss after mini-batch   800: 0.813\n",
            "Loss after mini-batch   850: 0.835\n",
            "Loss after mini-batch   900: 0.855\n",
            "Loss after mini-batch   950: 0.815\n",
            "Loss after mini-batch  1000: 0.830\n",
            "Loss after mini-batch  1050: 0.817\n",
            "Loss after mini-batch  1100: 0.824\n",
            "Loss after mini-batch  1150: 0.820\n",
            "Loss after mini-batch  1200: 0.819\n",
            "Loss after mini-batch  1250: 0.803\n",
            "Loss after mini-batch  1300: 0.796\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.808\n",
            "Loss after mini-batch   100: 0.788\n",
            "Loss after mini-batch   150: 0.804\n",
            "Loss after mini-batch   200: 0.806\n",
            "Loss after mini-batch   250: 0.823\n",
            "Loss after mini-batch   300: 0.803\n",
            "Loss after mini-batch   350: 0.824\n",
            "Loss after mini-batch   400: 0.802\n",
            "Loss after mini-batch   450: 0.829\n",
            "Loss after mini-batch   500: 0.821\n",
            "Loss after mini-batch   550: 0.833\n",
            "Loss after mini-batch   600: 0.795\n",
            "Loss after mini-batch   650: 0.756\n",
            "Loss after mini-batch   700: 0.775\n",
            "Loss after mini-batch   750: 0.800\n",
            "Loss after mini-batch   800: 0.779\n",
            "Loss after mini-batch   850: 0.798\n",
            "Loss after mini-batch   900: 0.819\n",
            "Loss after mini-batch   950: 0.778\n",
            "Loss after mini-batch  1000: 0.797\n",
            "Loss after mini-batch  1050: 0.783\n",
            "Loss after mini-batch  1100: 0.837\n",
            "Loss after mini-batch  1150: 0.785\n",
            "Loss after mini-batch  1200: 0.805\n",
            "Loss after mini-batch  1250: 0.783\n",
            "Loss after mini-batch  1300: 0.799\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 1: 75 %\n",
            "Precision :  0.7 Recall :  1.0 F-score :  0.8235294117647058\n",
            "FOLD 2\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 1.097\n",
            "Loss after mini-batch   100: 1.084\n",
            "Loss after mini-batch   150: 1.073\n",
            "Loss after mini-batch   200: 1.061\n",
            "Loss after mini-batch   250: 1.041\n",
            "Loss after mini-batch   300: 1.023\n",
            "Loss after mini-batch   350: 0.992\n",
            "Loss after mini-batch   400: 0.984\n",
            "Loss after mini-batch   450: 0.946\n",
            "Loss after mini-batch   500: 0.930\n",
            "Loss after mini-batch   550: 0.910\n",
            "Loss after mini-batch   600: 0.885\n",
            "Loss after mini-batch   650: 0.854\n",
            "Loss after mini-batch   700: 0.857\n",
            "Loss after mini-batch   750: 0.858\n",
            "Loss after mini-batch   800: 0.829\n",
            "Loss after mini-batch   850: 0.820\n",
            "Loss after mini-batch   900: 0.847\n",
            "Loss after mini-batch   950: 0.845\n",
            "Loss after mini-batch  1000: 0.787\n",
            "Loss after mini-batch  1050: 0.828\n",
            "Loss after mini-batch  1100: 0.806\n",
            "Loss after mini-batch  1150: 0.805\n",
            "Loss after mini-batch  1200: 0.816\n",
            "Loss after mini-batch  1250: 0.845\n",
            "Loss after mini-batch  1300: 0.810\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.820\n",
            "Loss after mini-batch   100: 0.798\n",
            "Loss after mini-batch   150: 0.799\n",
            "Loss after mini-batch   200: 0.807\n",
            "Loss after mini-batch   250: 0.786\n",
            "Loss after mini-batch   300: 0.805\n",
            "Loss after mini-batch   350: 0.795\n",
            "Loss after mini-batch   400: 0.802\n",
            "Loss after mini-batch   450: 0.814\n",
            "Loss after mini-batch   500: 0.792\n",
            "Loss after mini-batch   550: 0.792\n",
            "Loss after mini-batch   600: 0.811\n",
            "Loss after mini-batch   650: 0.807\n",
            "Loss after mini-batch   700: 0.791\n",
            "Loss after mini-batch   750: 0.810\n",
            "Loss after mini-batch   800: 0.790\n",
            "Loss after mini-batch   850: 0.786\n",
            "Loss after mini-batch   900: 0.824\n",
            "Loss after mini-batch   950: 0.790\n",
            "Loss after mini-batch  1000: 0.815\n",
            "Loss after mini-batch  1050: 0.796\n",
            "Loss after mini-batch  1100: 0.792\n",
            "Loss after mini-batch  1150: 0.787\n",
            "Loss after mini-batch  1200: 0.771\n",
            "Loss after mini-batch  1250: 0.797\n",
            "Loss after mini-batch  1300: 0.807\n",
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n",
            "Accuracy for fold 2: 75 %\n",
            "Precision :  0.7 Recall :  1.0 F-score :  0.8235294117647058\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 3 FOLDS\n",
            "Fold 0: 75.32738095238095 %\n",
            "Fold 1: 75.9375 %\n",
            "Fold 2: 75.38690476190476 %\n",
            "Average: 75.55059523809524 %\n"
          ]
        }
      ]
    }
  ]
}